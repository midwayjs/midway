"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[57304],{3905:(e,n,t)=>{t.d(n,{Zo:()=>p,kt:()=>k});var a=t(67294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=a.createContext({}),c=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},p=function(e){var n=c(e.components);return a.createElement(l.Provider,{value:n},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},d=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(t),d=r,k=u["".concat(l,".").concat(d)]||u[d]||m[d]||o;return t?a.createElement(k,i(i({ref:n},p),{},{components:t})):a.createElement(k,i({ref:n},p))}));function k(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=d;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[u]="string"==typeof e?e:r,i[1]=s;for(var c=2;c<o;c++)i[c]=t[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,t)}d.displayName="MDXCreateElement"},1208:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var a=t(87462),r=(t(67294),t(3905));const o={},i="Kafka",s={unversionedId:"extensions/kafka",id:"extensions/kafka",title:"Kafka",description:"In the architecture of complex systems, event streams are a crucial part, including capturing data in real-time from event sources (databases, sensors, mobile devices, etc.) as event streams, persisting event streams for easy retrieval, and processing and responding to event streams in real-time and retrospectively.",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/extensions/kafka.md",sourceDirName:"extensions",slug:"/extensions/kafka",permalink:"/en/docs/extensions/kafka",draft:!1,editUrl:"https://github.com/midwayjs/midway/tree/main/site/docs/extensions/kafka.md",tags:[],version:"current",frontMatter:{},sidebar:"component",previous:{title:"RabbitMQ",permalink:"/en/docs/extensions/rabbitmq"},next:{title:"MQTT",permalink:"/en/docs/extensions/mqtt"}},l={},c=[{value:"Basic Concepts",id:"basic-concepts",level:2},{value:"Install Dependencies",id:"install-dependencies",level:2},{value:"Enable Component",id:"enable-component",level:2},{value:"Consumer",id:"consumer",level:2},{value:"Directory Structure",id:"directory-structure",level:3},{value:"Basic Configuration",id:"basic-configuration",level:3},{value:"Reuse Kafka Instance",id:"reuse-kafka-instance",level:3},{value:"Consumer Implementation",id:"consumer-implementation",level:3},{value:"Message Context",id:"message-context",level:3},{value:"Producer",id:"producer",level:2},{value:"Basic Configuration",id:"basic-configuration-1",level:3},{value:"Using Producer",id:"using-producer",level:3},{value:"Admin",id:"admin",level:2},{value:"Basic Configuration",id:"basic-configuration-2",level:3},{value:"Using Admin",id:"using-admin",level:3},{value:"Component Logging",id:"component-logging",level:2},{value:"Access KafkaJS Module",id:"access-kafkajs-module",level:2},{value:"Warning About Partitions",id:"warning-about-partitions",level:2},{value:"Reference Documentation",id:"reference-documentation",level:2}],p={toc:c},u="wrapper";function m(e){let{components:n,...t}=e;return(0,r.kt)(u,(0,a.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"kafka"},"Kafka"),(0,r.kt)("p",null,"In the architecture of complex systems, event streams are a crucial part, including capturing data in real-time from event sources (databases, sensors, mobile devices, etc.) as event streams, persisting event streams for easy retrieval, and processing and responding to event streams in real-time and retrospectively."),(0,r.kt)("p",null,"Applicable to industries such as payment and financial transactions, implementing tracking and monitoring of automotive information flow, capturing and analyzing IoT data, etc."),(0,r.kt)("p",null,"In Midway, we provide the ability to subscribe to Kafka to meet such user needs."),(0,r.kt)("p",null,"Related Information:"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Subscription Service")),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Description"),(0,r.kt)("th",{parentName:"tr",align:null}))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Available for standard projects"),(0,r.kt)("td",{parentName:"tr",align:null},"\u2705")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Available for Serverless"),(0,r.kt)("td",{parentName:"tr",align:null},"\u274c")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Available for integrated projects"),(0,r.kt)("td",{parentName:"tr",align:null},"\u2705")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Includes standalone main framework"),(0,r.kt)("td",{parentName:"tr",align:null},"\u2705")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Includes standalone logging"),(0,r.kt)("td",{parentName:"tr",align:null},"\u2705")))),(0,r.kt)("h2",{id:"basic-concepts"},"Basic Concepts"),(0,r.kt)("p",null,"Distributed stream processing platform"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Publish-subscribe (stream) information"),(0,r.kt)("li",{parentName:"ul"},"Fault-tolerant (failover) storage of information (streams), storing event streams"),(0,r.kt)("li",{parentName:"ul"},"Process event streams as they occur")),(0,r.kt)("p",null,"Understanding Producer"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Publish messages to one or more topics.")),(0,r.kt)("p",null,"Understanding Consumer"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Subscribe to one or more topics and process the generated information.")),(0,r.kt)("p",null,"Understanding Stream API"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Acts as a stream processor, consuming input streams from one or more topics and producing an output stream to one or more output topics, effectively transforming input streams into output streams.")),(0,r.kt)("p",null,"Understanding Broker"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Published messages are stored in a set of servers called a Kafka cluster. Each server in the cluster is a broker. Consumers can subscribe to one or more topics and pull data from brokers to consume these published messages.")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://kafka.apache.org/images/streams-and-tables-p1_p4.png",alt:"image.png"})),(0,r.kt)("admonition",{type:"tip"},(0,r.kt)("p",{parentName:"admonition"},"From v3.19, the Kafka component has been refactored, and the configuration and usage methods of the Kafka component have changed significantly from before. The original usage method is compatible, but the documentation is no longer retained.")),(0,r.kt)("h2",{id:"install-dependencies"},"Install Dependencies"),(0,r.kt)("p",null,"Install the ",(0,r.kt)("inlineCode",{parentName:"p"},"@midwayjs/kafka")," module."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ npm i @midwayjs/kafka --save\n")),(0,r.kt)("p",null,"Or add the following dependency to ",(0,r.kt)("inlineCode",{parentName:"p"},"package.json")," and reinstall."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "dependencies": {\n    "@midwayjs/kafka": "^3.0.0",\n    // ...\n  }\n}\n')),(0,r.kt)("h2",{id:"enable-component"},"Enable Component"),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"@midwayjs/kafka")," can be used as a standalone main framework."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/configuration.ts\nimport { Configuration } from '@midwayjs/core';\nimport * as kafka from '@midwayjs/kafka';\n\n@Configuration({\n  imports: [\n    kafka\n  ],\n  // ...\n})\nexport class MainConfiguration {\n  async onReady() {\n        // ...\n  }\n}\n")),(0,r.kt)("p",null,"It can also be attached to other main frameworks, such as ",(0,r.kt)("inlineCode",{parentName:"p"},"@midwayjs/koa"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/configuration.ts\nimport { Configuration } from '@midwayjs/core';\nimport * as koa from '@midwayjs/koa';\nimport * as kafka from '@midwayjs/kafka';\n\n@Configuration({\n  imports: [\n    koa,\n    kafka\n  ],\n  // ...\n})\nexport class MainConfiguration {\n  async onReady() {\n        // ...\n  }\n}\n")),(0,r.kt)("p",null,"Since Kafka is divided into ",(0,r.kt)("strong",{parentName:"p"},"Consumer")," and ",(0,r.kt)("strong",{parentName:"p"},"Producer")," parts, both can be used independently, and we will introduce them separately."),(0,r.kt)("h2",{id:"consumer"},"Consumer"),(0,r.kt)("h3",{id:"directory-structure"},"Directory Structure"),(0,r.kt)("p",null,"We usually place consumers in the consumer directory, such as ",(0,r.kt)("inlineCode",{parentName:"p"},"src/consumer/user.consumer.ts"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"\u279c  my_midway_app tree\n.\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 consumer\n\u2502   \u2502   \u2514\u2500\u2500 user.consumer.ts\n\u2502   \u251c\u2500\u2500 interface.ts\n\u2502   \u2514\u2500\u2500 service\n\u2502       \u2514\u2500\u2500 user.service.ts\n\u251c\u2500\u2500 test\n\u251c\u2500\u2500 package.json\n\u2514\u2500\u2500 tsconfig.json\n")),(0,r.kt)("h3",{id:"basic-configuration"},"Basic Configuration"),(0,r.kt)("p",null,"We can configure multiple consumers through the ",(0,r.kt)("inlineCode",{parentName:"p"},"consumer")," field and the ",(0,r.kt)("inlineCode",{parentName:"p"},"@KafkaConsumer")," decorator."),(0,r.kt)("p",null,"For example, ",(0,r.kt)("inlineCode",{parentName:"p"},"sub1")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"sub2")," below are two different consumers."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/config/config.default\nexport default {\n  kafka: {\n    consumer: {\n      sub1: {\n        // ...\n      },\n      sub2: {\n        // ...\n      },\n    }\n  }\n}\n")),(0,r.kt)("p",null,"The simplest consumer configuration requires several fields: Kafka connection configuration, consumer configuration, and subscription configuration."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/config/config.default\nexport default {\n  kafka: {\n    consumer: {\n      sub1: {\n        connectionOptions: {\n          // ...\n        },\n        consumerOptions: {\n          // ...\n        },\n        subscribeOptions: {\n          // ...\n        },\n      },\n    }\n  }\n}\n")),(0,r.kt)("p",null,"For example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/config/config.default\nexport default {\n  kafka: {\n    consumer: {\n      sub1: {\n        connectionOptions: {\n          clientId: 'my-app',\n          brokers: ['localhost:9092'],\n        },\n        consumerOptions: {\n          groupId: 'groupId-test-1',\n        },\n        subscribeOptions: {\n          topics: ['topic-test-1'],\n        }\n      },\n    }\n  }\n}\n")),(0,r.kt)("p",null,"Complete configurable parameters include:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"connectionOptions"),": Kafka connection configuration, i.e., parameters for ",(0,r.kt)("inlineCode",{parentName:"li"},"new Kafka(consumerOptions)")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"consumerOptions"),": Kafka consumer configuration, i.e., parameters for ",(0,r.kt)("inlineCode",{parentName:"li"},"kafka.consumer(consumerOptions)")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"subscribeOptions"),": Kafka subscription configuration, i.e., parameters for ",(0,r.kt)("inlineCode",{parentName:"li"},"consumer.subscribe(subscribeOptions)")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"consumerRunConfig"),": Consumer run configuration, i.e., parameters for ",(0,r.kt)("inlineCode",{parentName:"li"},"consumer.run(consumerRunConfig)"))),(0,r.kt)("p",null,"For detailed explanations of these parameters, refer to the ",(0,r.kt)("a",{parentName:"p",href:"https://kafka.js.org/docs/consuming"},"KafkaJS Consumer")," documentation."),(0,r.kt)("h3",{id:"reuse-kafka-instance"},"Reuse Kafka Instance"),(0,r.kt)("p",null,"If you need to reuse a Kafka instance, you can specify it through the ",(0,r.kt)("inlineCode",{parentName:"p"},"kafkaInstanceRef")," field."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/config/config.default\nexport default {\n  kafka: {\n    consumer: {\n      sub1: {\n        connectionOptions: {\n          clientId: 'my-app',\n          brokers: ['localhost:9092'],\n        },\n        consumerOptions: {\n          groupId: 'groupId-test-1',\n        },\n        subscribeOptions: {\n          topics: ['topic-test-1'],\n        }\n      },\n      sub2: {\n        kafkaInstanceRef: 'sub1',\n        consumerOptions: {\n          groupId: 'groupId-test-2',\n        },\n        subscribeOptions: {\n          topics: ['topic-test-2'],\n        }\n      }\n    }\n  }\n}\n")),(0,r.kt)("p",null,"Note that ",(0,r.kt)("inlineCode",{parentName:"p"},"sub1")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"sub2")," above are two different consumers, but they share the same Kafka instance, and ",(0,r.kt)("inlineCode",{parentName:"p"},"sub2"),"'s ",(0,r.kt)("inlineCode",{parentName:"p"},"groupId")," needs to be different from ",(0,r.kt)("inlineCode",{parentName:"p"},"sub1"),"."),(0,r.kt)("p",null,"The Kafka SDK writing is similar to the following:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"const kafka = new Kafka({\n  clientId: 'my-app',\n  brokers: ['localhost:9092'],\n});\n\nconst consumer1 = kafka.consumer({ groupId: 'groupId-test-1' });\nconst consumer2 = kafka.consumer({ groupId: 'groupId-test-2' });\n")),(0,r.kt)("h3",{id:"consumer-implementation"},"Consumer Implementation"),(0,r.kt)("p",null,"We can provide a standard consumer implementation in the directory, such as ",(0,r.kt)("inlineCode",{parentName:"p"},"src/consumer/sub1.consumer.ts"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/consumer/sub1.consumer.ts\nimport { KafkaConsumer, IKafkaConsumer, EachMessagePayload } from '@midwayjs/kafka';\n\n@KafkaConsumer('sub1')\nclass Sub1Consumer implements IKafkaConsumer {\n  async eachMessage(payload: EachMessagePayload) {\n    // ...\n  }\n}\n")),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"sub1")," is the consumer name, using the ",(0,r.kt)("inlineCode",{parentName:"p"},"sub1")," consumer in the configuration."),(0,r.kt)("p",null,"You can also implement the ",(0,r.kt)("inlineCode",{parentName:"p"},"eachBatch")," method to process batch messages."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/consumer/sub1.consumer.ts\nimport { KafkaConsumer, IKafkaConsumer, EachBatchPayload } from '@midwayjs/kafka';\n\n@KafkaConsumer('sub1')\nclass Sub1Consumer implements IKafkaConsumer {\n  async eachBatch(payload: EachBatchPayload) {\n    // ...\n  }\n}\n")),(0,r.kt)("h3",{id:"message-context"},"Message Context"),(0,r.kt)("p",null,"Like other message subscription mechanisms, the message itself is passed through the ",(0,r.kt)("inlineCode",{parentName:"p"},"Context")," field."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/consumer/sub1.consumer.ts\nimport { KafkaConsumer, IKafkaConsumer, EachMessagePayload, Context } from '@midwayjs/kafka';\nimport { Inject } from '@midwayjs/core';\n\n@KafkaConsumer('sub1')\nclass Sub1Consumer implements IKafkaConsumer {\n\n  @Inject()\n  ctx: Context;\n\n  async eachMessage(payload: EachMessagePayload) {\n    // ...\n  }\n}\n")),(0,r.kt)("p",null,"The ",(0,r.kt)("inlineCode",{parentName:"p"},"Context")," field includes several properties:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Property"),(0,r.kt)("th",{parentName:"tr",align:null},"Type"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"ctx.payload"),(0,r.kt)("td",{parentName:"tr",align:null},"EachMessagePayload, EachBatchPayload"),(0,r.kt)("td",{parentName:"tr",align:null},"Message content")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"ctx.consumer"),(0,r.kt)("td",{parentName:"tr",align:null},"Consumer"),(0,r.kt)("td",{parentName:"tr",align:null},"Consumer instance")))),(0,r.kt)("p",null,"You can call Kafka's API through ",(0,r.kt)("inlineCode",{parentName:"p"},"ctx.consumer"),", such as ",(0,r.kt)("inlineCode",{parentName:"p"},"ctx.consumer.commitOffsets")," to manually commit offsets or ",(0,r.kt)("inlineCode",{parentName:"p"},"ctx.consumer.pause")," to pause consumption."),(0,r.kt)("h2",{id:"producer"},"Producer"),(0,r.kt)("h3",{id:"basic-configuration-1"},"Basic Configuration"),(0,r.kt)("p",null,"Service producers also need to create instances, and the configuration itself uses the ",(0,r.kt)("a",{parentName:"p",href:"/docs/service_factory"},"Service Factory")," design pattern."),(0,r.kt)("p",null,"The configuration is as follows:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/config/config.default\nexport default {\n  kafka: {\n    producer: {\n      clients: {\n        pub1: {\n          // ...\n        },\n        pub2: {\n          // ...\n        }\n      }\n    }\n  }\n}\n")),(0,r.kt)("p",null,"Each Producer instance's configuration also includes ",(0,r.kt)("inlineCode",{parentName:"p"},"connectionOptions")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"producerOptions"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/config/config.default\nexport default {\n  kafka: {\n    producer: {\n      clients: {\n        pub1: {\n          connectionOptions: {\n            clientId: 'my-app',\n            brokers: ['localhost:9092'],\n          },\n          producerOptions: {\n            // ...\n          }\n        }\n      }\n    }\n  }\n}\n")),(0,r.kt)("p",null,"For specific parameters, refer to the ",(0,r.kt)("a",{parentName:"p",href:"https://kafka.js.org/docs/producing"},"KafkaJS Producer")," documentation."),(0,r.kt)("p",null,"Additionally, since Kafka Consumer and Producer can both be created from the same Kafka instance, they can reuse the same Kafka instance."),(0,r.kt)("p",null,"If the Producer is created after the Consumer, it can also reuse the Kafka instance using the ",(0,r.kt)("inlineCode",{parentName:"p"},"kafkaInstanceRef")," field."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/config/config.default\nexport default {\n  kafka: {\n    consumer: {\n      sub1: {\n        connectionOptions: {\n          clientId: 'my-app',\n          brokers: ['localhost:9092'],\n        },\n      }\n    },\n    producer: {\n      clients: {\n        pub1: {\n          kafkaInstanceRef: 'sub1',\n        }\n      }\n    }\n  }\n}\n")),(0,r.kt)("h3",{id:"using-producer"},"Using Producer"),(0,r.kt)("p",null,"There is no default instance for Producer. Since the service factory design pattern is used, it can be injected through ",(0,r.kt)("inlineCode",{parentName:"p"},"@InjectClient()"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/service/user.service.ts\nimport { Provide, InjectClient } from '@midwayjs/core';\nimport { KafkaProducerFactory, Producer } from '@midwayjs/kafka';\n\n@Provide()\nexport class UserService {\n  \n  @InjectClient(KafkaProducerFactory, 'pub1')\n  producer: Producer;\n  \n  async invoke() {\n    await this.producer.send({\n      topic: 'topic-test-1',\n      messages: [{ key: 'message-key1', value: 'hello consumer 11 !' }],\n    });\n  }\n}\n")),(0,r.kt)("h2",{id:"admin"},"Admin"),(0,r.kt)("p",null,"Kafka's Admin functionality can be used to create, delete, view topics, view configurations, and ACLs, etc."),(0,r.kt)("h3",{id:"basic-configuration-2"},"Basic Configuration"),(0,r.kt)("p",null,"Like Producer, Admin also uses the service factory design pattern."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/config/config.default\nexport default {\n  kafka: {\n    admin: {\n      clients: {\n        admin1: {\n          // ...\n        }\n      }\n    }\n  }\n}\n")),(0,r.kt)("p",null,"Similarly, Admin can also reuse the Kafka instance."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/config/config.default\nexport default {\n  kafka: {\n    consumer: {\n      sub1: {\n        connectionOptions: {\n          clientId: 'my-app',\n          brokers: ['localhost:9092'],\n        },\n      }\n    },\n    admin: {\n      clients: {\n        admin1: {\n          kafkaInstanceRef: 'sub1',\n        }\n      }\n    }\n  }\n}\n")),(0,r.kt)("h3",{id:"using-admin"},"Using Admin"),(0,r.kt)("p",null,"There is no default instance for Admin. Since the service factory design pattern is used, it can be injected through ",(0,r.kt)("inlineCode",{parentName:"p"},"@InjectClient()"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/service/admin.service.ts\nimport { Provide, InjectClient } from '@midwayjs/core';\nimport { KafkaAdminFactory, Admin } from '@midwayjs/kafka';\n\n@Provide()\nexport class AdminService {\n  \n  @InjectClient(KafkaAdminFactory, 'admin1')\n  admin: Admin;\n}\n")),(0,r.kt)("p",null,"For more Admin usage methods, refer to the ",(0,r.kt)("a",{parentName:"p",href:"https://kafka.js.org/docs/admin"},"KafkaJS Admin")," documentation."),(0,r.kt)("h2",{id:"component-logging"},"Component Logging"),(0,r.kt)("p",null,"The Kafka component uses the ",(0,r.kt)("inlineCode",{parentName:"p"},"kafkaLogger")," log by default, which will record ",(0,r.kt)("inlineCode",{parentName:"p"},"ctx.logger")," in ",(0,r.kt)("inlineCode",{parentName:"p"},"midway-kafka.log"),"."),(0,r.kt)("p",null,"You can modify it through configuration."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/config/config.default\nexport default {\n  midwayLogger: {\n    clients: {\n      kafkaLogger: {\n        fileLogName: 'midway-kafka.log',\n      },\n    },\n  },\n}\n")),(0,r.kt)("p",null,"The output format of this log can also be configured separately."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"export default {\n  kafka: {\n    // ...\n    contextLoggerFormat: info => {\n      const { jobId, from } = info.ctx;\n      return `${info.timestamp} ${info.LEVEL} ${info.pid} ${info.message}`;\n    },\n  }\n}\n")),(0,r.kt)("h2",{id:"access-kafkajs-module"},"Access KafkaJS Module"),(0,r.kt)("p",null,"The KafkaJS module can be accessed through the ",(0,r.kt)("inlineCode",{parentName:"p"},"KafkaJS")," field of ",(0,r.kt)("inlineCode",{parentName:"p"},"@midwayjs/kafka"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"import { KafkaJS } from '@midwayjs/kafka';\n\nconst { ConfigResourceTypes } = KafkaJS;\n// ...\n")),(0,r.kt)("h2",{id:"warning-about-partitions"},"Warning About Partitions"),(0,r.kt)("p",null,"If you are using KafkaJS version v2.0.0, you may see the following warning:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"2024-11-04 23:47:28.228 WARN 31729 KafkaJS v2.0.0 switched default partitioner. To retain the same partitioning behavior as in previous versions, create the producer with the option \"createPartitioner: Partitioners.LegacyPartitioner\". See the migration guide at https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner for details. Silence this warning by setting the environment variable \"KAFKAJS_NO_PARTITIONER_WARNING=1\" { timestamp: '2024-11-04T15:47:28.228Z', logger: 'kafkajs' }\n")),(0,r.kt)("p",null,"This warning is due to KafkaJS version v2.0.0 using a new partitioner by default. If you accept the new partitioner behavior but want to turn off this warning message, you can eliminate this warning by setting the environment variable ",(0,r.kt)("inlineCode",{parentName:"p"},"KAFKAJS_NO_PARTITIONER_WARNING=1"),"."),(0,r.kt)("p",null,"Or explicitly declare the partitioner."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-typescript"},"// src/config/config.default\nimport { KafkaJS } from '@midwayjs/kafka';\nconst { Partitioners } = KafkaJS;\n\nexport default {\n  kafka: {\n    producer: {\n      clients: {\n        pub1: {\n          // ...\n          producerOptions: {\n            createPartitioner: Partitioners.DefaultPartitioner,\n            // ...\n            createPartitioner: Partitioners.LegacyPartitioner,\n          },\n        },\n      },\n    },\n  }\n}\n")),(0,r.kt)("p",null,"It is recommended to check the KafkaJS v2.0.0 ",(0,r.kt)("a",{parentName:"p",href:"https://kafka.js.org/docs/migration-guide-v2.0.0#producer-new-default-partitioner"},"migration guide")," for more details."),(0,r.kt)("h2",{id:"reference-documentation"},"Reference Documentation"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://kafka.js.org/docs/introduction"},"KafkaJS")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://kafka.apache.org/intro"},"Apache Kafka Official Website"))))}m.isMDXComponent=!0}}]);